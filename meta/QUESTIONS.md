# The Honest Questions

Simon Willison's [assessment](https://simonwillison.net/2026/Feb/7/software-factory/) of the Software Factory concept provides essential design constraints disguised as skepticism.

## Cost

How do you prove agent-written code works when both the implementation and the tests are generated by agents? This is, as Willison frames it, potentially the most consequential question in software development right now. StrongDM's answer — scenario-based holdout sets evaluated by independent judges — is the most impressive element, but it remains an open problem. What about the economics? StrongDM reports $1,000/day in token costs per engineer. At $20,000/month overhead, the value proposition depends entirely on output velocity exceeding that of human engineers by a wide margin. And what happens to competitive moats when any organization can clone features in hours?

## Readiness

Before asking "does it make sense to have coding agents updating my codebase without review," ask a harder question: can your system *prove it works*?

The software factory approach depends on a verifiability constraint. Agent output must be validated against something close to production reality — what the [Digital Twin Universe](../techniques/digital-twin-universe.md) technique calls a behavioral clone of the real environment. In the mature end state, the goal is not a codebase that passes tests. It is a system that continuously proves it works: every deploy, every workflow, every user journey evidenced by telemetry, assertions, synthetic checks, and regression detectors.

Most existing codebases are nowhere near this. They lack the observability and reproducibility stack that the approach requires. The honest starting point is not "adopt agents" but "build proving loops" — the infrastructure that makes correctness demonstrable:

- **Service-level invariants** — automated checks that critical business rules hold after every change
- **Golden signals** — latency, error rate, throughput, and saturation monitored continuously with alerting thresholds
- **Synthetic journeys** — automated user-flow simulations running against production or near-production environments
- **Regression detectors** — statistical baselines that flag behavioral drift before users notice
- **SLO burn correlation** — linking deployment events to error-budget consumption so the system can attribute degradation to specific changes

Organizations that build these proving loops first will find that agents slot in naturally — the validation infrastructure already exists. Organizations that skip straight to agent adoption will discover, expensively, that they cannot distinguish agent-written regressions from pre-existing fragility.

For a structured assessment of where a codebase stands, Factory.ai's [Agent Readiness Model](maturity-model.md) provides a maturity framework with five levels and nine technical pillars. The pillars map directly to the formula: Seed, Validation, and Feedback Loop — a codebase that scores well across them is one where the proving loops are already in place.
