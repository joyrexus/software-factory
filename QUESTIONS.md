# The Honest Questions

Simon Willison's [assessment](https://simonwillison.net/2026/Feb/7/software-factory/) of the Software Factory concept provides essential design constraints disguised as skepticism.

## Cost

How do you prove agent-written code works when both the implementation and the tests are generated by agents? This is, as Willison frames it, potentially the most consequential question in software development right now. StrongDM's answer — scenario-based holdout sets evaluated by independent judges — is the most impressive element, but it remains an open problem. What about the economics? StrongDM reports $1,000/day in token costs per engineer. At $20,000/month overhead, the value proposition depends entirely on output velocity exceeding that of human engineers by a wide margin. And what happens to competitive moats when any organization can clone features in hours?

## Readiness

Before asking "can agents write code?", ask "is this codebase ready for agents to work in?"

The Agent Readiness Model provides a conceptual framework for evaluating whether a codebase is prepared for autonomous agent operation. Rather than measuring agent capability, it measures the environment agents operate within — the same insight Larson's [Compound Engineering](SOURCES.md#compound-engineering) framework arrives at from a different direction.

### Maturity Levels

The model defines five levels of codebase maturity for agent operation:

| Level | Name | Description |
|-------|------|-------------|
| L1 | Functional | Code runs but requires manual setup, no automated validation |
| L2 | Documented | Basic docs and some automation exist |
| L3 | Standardized | Processes defined, documented, enforced through automation — the minimum bar for production-grade autonomous operation |
| L4 | Optimized | Fast feedback loops, data-driven improvement, continuous measurement |
| L5 | Autonomous | Self-improving systems with sophisticated orchestration |

Advancement requires passing 80% of criteria at the current level and all previous levels. This gating rule prevents organizations from claiming L4 optimization while lacking L2 documentation.

### Technical Pillars

Nine pillars define what gets measured at each level:

1. **Style & Validation** — automated enforcement of code standards and formatting
2. **Build System** — reproducible, deterministic builds that agents can invoke without human guidance
3. **Testing** — layered test suites (unit, integration, end-to-end) with clear pass/fail signals
4. **Documentation** — structured context that agents can discover and consume
5. **Development Environment** — consistent, reproducible setup that eliminates "works on my machine"
6. **Debugging & Observability** — structured logs, traces, and error reporting agents can interpret
7. **Security** — automated scanning, dependency auditing, and policy enforcement
8. **Task Discovery** — structured backlogs and specifications agents can parse into actionable work
9. **Product & Experimentation** — feature flags, A/B testing, and rollback mechanisms for safe deployment

### Connection to the Formula

The pillars map directly to the formula — Seed, Validation, Feedback Loop. Style & Validation, Build System, and Testing make the **Validation** stage reliable. Documentation, Development Environment, and Task Discovery ensure **Seeds** are well-formed. Debugging & Observability, Security, and Product & Experimentation close the **Feedback Loop** by making outcomes measurable and recoverable. A codebase that scores well across all nine pillars is one where the formula can operate with confidence.

---

These aren't objections. They're the problems that the [principles](principles/INDEX.md), [techniques](techniques/INDEX.md), and [components](components/INDEX.md) in this knowledge base must solve.
